{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d2b5657",
   "metadata": {},
   "source": [
    "# LightGBM Booking Probability Prediction with Optuna Tuning\n",
    "This notebook demonstrates how to train a LightGBM model that predicts the probability a hotel will be booked. We use Optuna to tune hyperparameters with cross‑validation, evaluate the tuned model, and optionally explain it with SHAP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3368288c",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "556541be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/fenics-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003221a",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "Adjust the paths and parameters below to match your environment and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55314762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === File paths ===\n",
    "TRAIN_CSV = '/Users/s.broos/Documents/DMT_data/training_set_VU_DM.csv'            # TODO: replace with your training file\n",
    "OUT_FEAT_CSV = '/Users/s.broos/Documents/DMT_data/features_with_cf_score.csv'  # TODO: replace with your engineered‑features file\n",
    "\n",
    "# === Column names ===\n",
    "LABEL_CLICK = 'click_bool'\n",
    "LABEL_BOOK = 'booking_bool'\n",
    "LEAKS = ['position', 'gross_bookings_usd']     # columns that leak target information\n",
    "REMOVE_COLS = []                               # extra columns you want dropped\n",
    "\n",
    "# === Sampling ===\n",
    "SAMPLE_FRAC = 1.0   # set <1.0 for quick tests\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a4e2e",
   "metadata": {},
   "source": [
    "## 3. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f147ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_light(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Simple numeric imputation with medians.\"\"\"\n",
    "    df = df.copy()\n",
    "    num_cols = df.select_dtypes('number').columns\n",
    "    df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef80ec3",
   "metadata": {},
   "source": [
    "## 4. Load & prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf7b0e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full feature matrix shape: (4958347, 66)\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Load raw data\n",
    "df_full = pd.read_csv(TRAIN_CSV)\n",
    "df_full = engineer_light(df_full)\n",
    "\n",
    "# 4.2 Labels\n",
    "y = df_full[LABEL_BOOK]\n",
    "\n",
    "# 4.3 Base feature matrix\n",
    "drop_cols = [LABEL_CLICK, LABEL_BOOK] + LEAKS + ['date_time']\n",
    "X_base = df_full.drop(columns=drop_cols)\n",
    "X_base = X_base.drop(columns=[c for c in REMOVE_COLS if c in X_base.columns])\n",
    "\n",
    "# 4.4 Merge candidate engineered features\n",
    "cand = pd.read_csv(OUT_FEAT_CSV)\n",
    "X_full = X_base.merge(cand, on=['srch_id', 'prop_id'], how='left')\n",
    "\n",
    "# 4.5 (Optional) sample for speed\n",
    "X_full = X_full.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE)\n",
    "y = y.loc[X_full.index].reset_index(drop=True)\n",
    "X_full = X_full.reset_index(drop=True)\n",
    "\n",
    "print(f'Full feature matrix shape: {X_full.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc458f",
   "metadata": {},
   "source": [
    "### 4.6 Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e367e698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (3470842, 66),  Test size: (1487505, 66)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y,\n",
    "    test_size=0.3,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "print(f'Train size: {X_train.shape},  Test size: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5642d80",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter optimisation with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73dcd4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:40:58,313] A new study created in memory with name: no-name-b81805c3-2bbe-45b7-a37b-6f13fdc3989d\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 40 rounds\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's auc: 0.913346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.913346:   3%|▎         | 1/30 [01:30<43:51, 90.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:42:29,046] Trial 1 finished with value: 0.913346239236184 and parameters: {'learning_rate': 0.16866861590324983, 'num_leaves': 147, 'min_child_samples': 28, 'feature_fraction': 0.7426796664344582, 'bagging_fraction': 0.6322446513624453}. Best is trial 1 with value: 0.913346239236184.\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid_0's auc: 0.917013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 5. Best value: 0.917013:   7%|▋         | 2/30 [01:45<21:21, 45.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:42:43,030] Trial 5 finished with value: 0.9170127134106214 and parameters: {'learning_rate': 0.13154554628525342, 'num_leaves': 43, 'min_child_samples': 36, 'feature_fraction': 0.7860362078243486, 'bagging_fraction': 0.9596818556624573}. Best is trial 5 with value: 0.9170127134106214.\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's auc: 0.916606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 5. Best value: 0.917013:  10%|█         | 3/30 [01:59<14:11, 31.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:42:57,973] Trial 0 finished with value: 0.9166062119961964 and parameters: {'learning_rate': 0.14469695775160363, 'num_leaves': 75, 'min_child_samples': 76, 'feature_fraction': 0.6670272887744738, 'bagging_fraction': 0.8013713764300585}. Best is trial 5 with value: 0.9170127134106214.\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "Early stopping, best iteration is:\n",
      "[182]\tvalid_0's auc: 0.917838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.917838:  13%|█▎        | 4/30 [02:53<17:33, 40.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:43:52,276] Trial 6 finished with value: 0.9178376959062575 and parameters: {'learning_rate': 0.0981004702785791, 'num_leaves': 54, 'min_child_samples': 99, 'feature_fraction': 0.8578905406944405, 'bagging_fraction': 0.8497728121836986}. Best is trial 6 with value: 0.9178376959062575.\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "Early stopping, best iteration is:\n",
      "[155]\tvalid_0's auc: 0.917918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.917918:  17%|█▋        | 5/30 [04:20<23:45, 57.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:45:18,494] Trial 8 finished with value: 0.9179182192726577 and parameters: {'learning_rate': 0.07134114782088614, 'num_leaves': 74, 'min_child_samples': 71, 'feature_fraction': 0.836872087862134, 'bagging_fraction': 0.7775885186661563}. Best is trial 8 with value: 0.9179182192726577.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.917918:  23%|██▎       | 7/30 [04:20<09:44, 25.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:45:18,586] Trial 11 pruned. Trial was pruned at iteration 44.\n",
      "[I 2025-05-14 17:45:18,752] Trial 10 pruned. Trial was pruned at iteration 142.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.917918:  27%|██▋       | 8/30 [04:43<08:59, 24.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 40 rounds\n",
      "[I 2025-05-14 17:45:41,529] Trial 12 pruned. Trial was pruned at iteration 0.\n",
      "Training until validation scores don't improve for 40 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.917918:  30%|███       | 9/30 [04:46<06:18, 18.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:45:45,266] Trial 13 pruned. Trial was pruned at iteration 3.\n",
      "Training until validation scores don't improve for 40 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.917918:  33%|███▎      | 10/30 [05:11<06:39, 19.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:46:09,604] Trial 14 pruned. Trial was pruned at iteration 28.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.917918:  37%|███▋      | 11/30 [05:13<04:38, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:46:12,262] Trial 16 pruned. Trial was pruned at iteration 0.\n",
      "Training until validation scores don't improve for 40 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.917918:  40%|████      | 12/30 [05:35<05:03, 16.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:46:34,092] Trial 17 pruned. Trial was pruned at iteration 3.\n",
      "Training until validation scores don't improve for 40 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.917918:  43%|████▎     | 13/30 [05:42<03:52, 13.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:46:40,419] Trial 18 pruned. Trial was pruned at iteration 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.917918:  47%|████▋     | 14/30 [05:51<03:18, 12.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:46:49,849] Trial 15 pruned. Trial was pruned at iteration 65.\n",
      "Training until validation scores don't improve for 40 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.917918:  50%|█████     | 15/30 [05:58<02:42, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:46:57,133] Trial 19 pruned. Trial was pruned at iteration 2.\n",
      "Training until validation scores don't improve for 40 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.917918:  53%|█████▎    | 16/30 [06:08<02:25, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:47:06,421] Trial 20 pruned. Trial was pruned at iteration 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.917918:  57%|█████▋    | 17/30 [06:16<02:05,  9.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:47:14,386] Trial 21 pruned. Trial was pruned at iteration 0.\n",
      "Early stopping, best iteration is:\n",
      "[455]\tvalid_0's auc: 0.918726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.917918:  60%|██████    | 18/30 [06:25<01:54,  9.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:47:23,651] Trial 22 pruned. Trial was pruned at iteration 0.\n",
      "Early stopping, best iteration is:\n",
      "[452]\tvalid_0's auc: 0.918521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.917918:  63%|██████▎   | 19/30 [06:33<01:40,  9.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:47:31,853] Trial 23 pruned. Trial was pruned at iteration 0.\n",
      "Early stopping, best iteration is:\n",
      "[221]\tvalid_0's auc: 0.918347\n",
      "Training until validation scores don't improve for 40 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 8. Best value: 0.917918:  67%|██████▋   | 20/30 [06:48<01:48, 10.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:47:46,917] Trial 24 pruned. Trial was pruned at iteration 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.918726:  70%|███████   | 21/30 [06:49<01:11,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:47:48,123] Trial 2 finished with value: 0.9187261423038111 and parameters: {'learning_rate': 0.03273981313509022, 'num_leaves': 92, 'min_child_samples': 34, 'feature_fraction': 0.8346045239803968, 'bagging_fraction': 0.8405963158646577}. Best is trial 2 with value: 0.9187261423038111.\n",
      "Training until validation scores don't improve for 40 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.918726:  73%|███████▎  | 22/30 [06:55<00:58,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:47:53,604] Trial 9 finished with value: 0.9183466615080449 and parameters: {'learning_rate': 0.049430321837007345, 'num_leaves': 162, 'min_child_samples': 97, 'feature_fraction': 0.7374864389952597, 'bagging_fraction': 0.769328188942748}. Best is trial 2 with value: 0.9187261423038111.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.918726:  77%|███████▋  | 23/30 [06:56<00:38,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:47:55,209] Trial 3 finished with value: 0.9185207341857595 and parameters: {'learning_rate': 0.03724026891002677, 'num_leaves': 108, 'min_child_samples': 98, 'feature_fraction': 0.8685425797682383, 'bagging_fraction': 0.7295036626644583}. Best is trial 2 with value: 0.9187261423038111.\n",
      "Training until validation scores don't improve for 40 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.918726:  80%|████████  | 24/30 [07:03<00:35,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:48:02,303] Trial 25 pruned. Trial was pruned at iteration 10.\n",
      "Training until validation scores don't improve for 40 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.918726:  83%|████████▎ | 25/30 [07:11<00:32,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 40 rounds\n",
      "[I 2025-05-14 17:48:09,550] Trial 26 pruned. Trial was pruned at iteration 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.918726:  83%|████████▎ | 25/30 [07:12<00:32,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:48:11,047] Trial 28 pruned. Trial was pruned at iteration 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.918726:  90%|█████████ | 27/30 [07:13<00:10,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:48:11,858] Trial 27 pruned. Trial was pruned at iteration 3.\n",
      "Training until validation scores don't improve for 40 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.918726:  93%|█████████▎| 28/30 [07:15<00:05,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:48:13,314] Trial 29 pruned. Trial was pruned at iteration 1.\n",
      "Early stopping, best iteration is:\n",
      "[714]\tvalid_0's auc: 0.91872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.918726:  97%|█████████▋| 29/30 [07:26<00:05,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:48:24,642] Trial 7 finished with value: 0.9187197118735517 and parameters: {'learning_rate': 0.0265228313275646, 'num_leaves': 50, 'min_child_samples': 63, 'feature_fraction': 0.8695287429670948, 'bagging_fraction': 0.667308395865633}. Best is trial 2 with value: 0.9187261423038111.\n",
      "Early stopping, best iteration is:\n",
      "[785]\tvalid_0's auc: 0.919291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.919291: 100%|██████████| 30/30 [07:57<00:00, 15.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 17:48:55,472] Trial 4 finished with value: 0.9192906198175425 and parameters: {'learning_rate': 0.02340256277784342, 'num_leaves': 72, 'min_child_samples': 69, 'feature_fraction': 0.6719019183257495, 'bagging_fraction': 0.8929756582704587}. Best is trial 4 with value: 0.9192906198175425.\n",
      "Best AUC (CV): 0.9192906198175425\n",
      "Best params : {'learning_rate': 0.02340256277784342, 'num_leaves': 72, 'min_child_samples': 69, 'feature_fraction': 0.6719019183257495, 'bagging_fraction': 0.8929756582704587}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Fast LightGBM + Optuna tuning block\n",
    "# ------------------------------------------------------------------\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ---------- fast-tune hyper-parameters ----------\n",
    "SAMPLE_FRAC   = 0.60      # more data\n",
    "NUM_BOOST_CAP = 1200      # more trees\n",
    "EARLY_STOP    = 40        # patience\n",
    "N_TRIALS      = 30        # more Optuna shots\n",
    "\n",
    "# ---------- one-time sampling for speed ----------\n",
    "X_tune, _, y_tune, _ = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=1 - SAMPLE_FRAC,\n",
    "    stratify=y_train,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# ---------- Optuna objective ----------\n",
    "def objective(trial):\n",
    "    # --- 80 / 20 validation split ---\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_tune, y_tune,\n",
    "        test_size=0.20,\n",
    "        stratify=y_tune,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    dtr  = lgb.Dataset(X_tr,  label=y_tr)\n",
    "    dval = lgb.Dataset(X_val, label=y_val, reference=dtr)\n",
    "\n",
    "    # --- compact search space ---\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"verbosity\": -1,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.2, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 255, log=True),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 20, 100),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
    "        \"bagging_freq\": 1,\n",
    "        # \"device_type\": \"gpu\",   # ← uncomment if you have CUDA\n",
    "    }\n",
    "\n",
    "    booster = lgb.train(\n",
    "        params,\n",
    "        dtr,\n",
    "        num_boost_round=NUM_BOOST_CAP,\n",
    "        valid_sets=[dval],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(EARLY_STOP),\n",
    "            LightGBMPruningCallback(trial, \"auc\"),\n",
    "            lgb.log_evaluation(period=0),   # silent\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    preds = booster.predict(X_val, num_iteration=booster.best_iteration)\n",
    "    return roc_auc_score(y_val, preds)\n",
    "\n",
    "# ---------- run the study ----------\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=N_TRIALS,\n",
    "    n_jobs=-1,                # use all available CPU cores\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"Best AUC (CV):\", study.best_value)\n",
    "print(\"Best params :\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4630c6",
   "metadata": {},
   "source": [
    "Best trial: 4. Best value: 0.919291: 100%|██████████| 30/30 [07:57<00:00, 15.91s/it]\n",
    "[I 2025-05-14 17:48:55,472] Trial 4 finished with value: 0.9192906198175425 and parameters: {'learning_rate': 0.02340256277784342, 'num_leaves': 72, 'min_child_samples': 69, 'feature_fraction': 0.6719019183257495, 'bagging_fraction': 0.8929756582704587}. Best is trial 4 with value: 0.9192906198175425.\n",
    "Best AUC (CV): 0.9192906198175425\n",
    "Best params : {'learning_rate': 0.02340256277784342, 'num_leaves': 72, 'min_child_samples': 69, 'feature_fraction': 0.6719019183257495, 'bagging_fraction': 0.8929756582704587}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4ab99",
   "metadata": {},
   "source": [
    "## 6. Train final model with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0104362",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 22\u001b[0m\n\u001b[1;32m     13\u001b[0m best_params\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_logloss\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: RANDOM_STATE,\n\u001b[1;32m     18\u001b[0m })\n\u001b[1;32m     20\u001b[0m best_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1200\u001b[39m          \u001b[38;5;66;03m# ← use the median/best from tuning\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m gbm_final \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdall\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 5. Save for production use\u001b[39;00m\n\u001b[1;32m     29\u001b[0m MODEL_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightgbm_full_final.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fenics-env/lib/python3.9/site-packages/lightgbm/engine.py:328\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_sets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m--> 328\u001b[0m         evaluation_result_list\u001b[38;5;241m.\u001b[39mextend(\u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    329\u001b[0m     evaluation_result_list\u001b[38;5;241m.\u001b[39mextend(booster\u001b[38;5;241m.\u001b[39meval_valid(feval))\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fenics-env/lib/python3.9/site-packages/lightgbm/basic.py:4408\u001b[0m, in \u001b[0;36mBooster.eval_train\u001b[0;34m(self, feval)\u001b[0m\n\u001b[1;32m   4376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21meval_train\u001b[39m(\n\u001b[1;32m   4377\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4378\u001b[0m     feval: Optional[Union[_LGBM_CustomEvalFunction, List[_LGBM_CustomEvalFunction]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4379\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_LGBM_BoosterEvalMethodResultType]:\n\u001b[1;32m   4380\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate for training data.\u001b[39;00m\n\u001b[1;32m   4381\u001b[0m \n\u001b[1;32m   4382\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4406\u001b[0m \u001b[38;5;124;03m        List with (train_dataset_name, eval_name, eval_result, is_higher_better) tuples.\u001b[39;00m\n\u001b[1;32m   4407\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inner_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_data_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/fenics-env/lib/python3.9/site-packages/lightgbm/basic.py:5185\u001b[0m, in \u001b[0;36mBooster.__inner_eval\u001b[0;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[1;32m   5182\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_inner_eval, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m   5183\u001b[0m tmp_out_len \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   5184\u001b[0m _safe_call(\n\u001b[0;32m-> 5185\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterGetEval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5186\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_out_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOINTER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_double\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5191\u001b[0m )\n\u001b[1;32m   5192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tmp_out_len\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_inner_eval:\n\u001b[1;32m   5193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length of eval results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============ FINAL TRAINING (all data) ============\n",
    "\n",
    "import lightgbm as lgb\n",
    "import joblib, os\n",
    "\n",
    "# 1. Merge every row you have\n",
    "X_full = pd.concat([X_train, X_test]).reset_index(drop=True)\n",
    "y_full = pd.concat([y_train, y_test]).reset_index(drop=True)\n",
    "dall   = lgb.Dataset(X_full, label=y_full)\n",
    "\n",
    "# 2. Params from Optuna\n",
    "best_params = study.best_params.copy()\n",
    "best_params.update({\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": [\"auc\", \"binary_logloss\"],\n",
    "    \"verbosity\": -1,\n",
    "    \"seed\": RANDOM_STATE,\n",
    "})\n",
    "\n",
    "cv_results = lgb.cv(\n",
    "    best_params,\n",
    "    dall,\n",
    "    num_boost_round=4000,          # hard cap\n",
    "    nfold=3,                       # 3 folds instead of 5\n",
    "    stratified=True,\n",
    "    shuffle=True,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(100),   # patience halved\n",
    "        lgb.log_evaluation(period=200),\n",
    "    ],\n",
    ")\n",
    "best_iter = len(cv_results[\"auc-mean\"])\n",
    "print(f\"\\nBest_iteration from CV: {best_iter}\")\n",
    "\n",
    "# 4. Fit one model on 100 % of the data\n",
    "gbm_final = lgb.train(\n",
    "    best_params,\n",
    "    dall,\n",
    "    num_boost_round=best_iter,\n",
    "    valid_sets=[dall],\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# 5. Save for production use\n",
    "MODEL_PATH = \"lightgbm_full_final.pkl\"\n",
    "joblib.dump(gbm_final, MODEL_PATH)\n",
    "print(f\"✅  Model saved to {os.path.abspath(MODEL_PATH)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c42b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ EVALUATION (OOF metrics) ============\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, log_loss,\n",
    "    brier_score_loss, accuracy_score, precision_score,\n",
    "    recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# 1. Collect OOF predictions from the cvbooster\n",
    "oof_pred = np.zeros(len(X_full))\n",
    "for booster, (_, val_idx) in zip(cv_results[\"cvbooster\"].boosters, cv_results[\"cvbooster\"].folds):\n",
    "    oof_pred[val_idx] = booster.predict(\n",
    "        X_full.iloc[val_idx],\n",
    "        num_iteration=booster.best_iteration\n",
    "    )\n",
    "\n",
    "# 2. Threshold → class labels (0.50 default)\n",
    "y_pred = (oof_pred >= 0.50).astype(int)\n",
    "\n",
    "# 3. Metrics table\n",
    "print(\"──────────  CV Out-of-Fold Metrics  ──────────\")\n",
    "print(f\"AUC-ROC           : {roc_auc_score(y_full, oof_pred):.4f}\")\n",
    "print(f\"PR-AUC            : {average_precision_score(y_full, oof_pred):.4f}\")\n",
    "print(f\"Log-loss          : {log_loss(y_full, oof_pred):.4f}\")\n",
    "print(f\"Brier score       : {brier_score_loss(y_full, oof_pred):.4f}\")\n",
    "print(\"---------------------------------------------\")\n",
    "print(f\"Accuracy (0.5)    : {accuracy_score(y_full, y_pred):.4f}\")\n",
    "print(f\"Precision (0.5)   : {precision_score(y_full, y_pred, zero_division=0):.4f}\")\n",
    "print(f\"Recall    (0.5)   : {recall_score(y_full, y_pred, zero_division=0):.4f}\")\n",
    "print(f\"F1-score  (0.5)   : {f1_score(y_full, y_pred, zero_division=0):.4f}\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_full, y_pred).ravel()\n",
    "print(f\"Confusion matrix  :  TN={tn}  FP={fp}  FN={fn}  TP={tp}\")\n",
    "print(\"──────────────────────────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6565924",
   "metadata": {},
   "source": [
    "## 7. Interpretability with SHAP\n",
    "Uncomment the summary‑plot line if you wish to visualise SHAP values inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1265136",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(gbm_final)\n",
    "shap_values = explainer.shap_values(X_test.fillna(0))\n",
    "\n",
    "# shap.summary_plot(shap_values, X_test, max_display=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0c6a7",
   "metadata": {},
   "source": [
    "---\n",
    "*End of notebook*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fenics-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
